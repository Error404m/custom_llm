# MrityunjayaGPT: A Custom Small Language Model (SLM)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![License](https://img.shields.io/badge/License-MIT-green)
![Parameter Count](https://img.shields.io/badge/Params-~15M-purple)

**MrityunjayaGPT** is a decoder-only Transformer model built entirely from scratch using PyTorch. Unlike pre-trained models accessed via APIs, every component of this Large Language Model (LLM)‚Äîfrom the self-attention mechanism to the training loop‚Äîwas coded by hand to understand the internal mechanics of Generative AI.

It is trained on the **TinyStories** dataset to demonstrate that a small model (SLM) can learn to speak coherent, grammatically correct English when trained on high-quality, simplified data.

---

## üìë Table of Contents
- [Project Overview](#-project-overview)
- [Technical Architecture](#-technical-architecture)
- [Dataset Details](#-dataset-details)
- [Installation & Setup](#-installation--setup)
- [Usage Guide](#-usage-guide)
- [Configuration & Hyperparameters](#-configuration--hyperparameters)
- [Sample Generation](#-sample-generation)
- [Theoretical Concepts](#-theoretical-concepts)
- [Credits](#-credits)

---

## üöÄ Project Overview

The goal of MrityunjayaGPT is to demystify the "black box" of LLMs. It implements a **Generative Pre-trained Transformer (GPT)** architecture similar to GPT-2 but scaled down for personal hardware.

**Key Features:**
* **Tokenization:** Implements Byte-Pair Encoding (BPE) using the GPT-2 tokenizer.
* **Custom Attention:** Manual implementation of Causal Multi-Head Self-Attention.
* **Optimization:** Uses AdamW optimizer, Gradient Clipping, and Cosine Learning Rate Decay.
* **Efficiency:** Features Mixed Precision Training (AMP) and Gradient Accumulation for efficient GPU usage.

---

## üß† Technical Architecture

MrityunjayaGPT follows the standard Transformer Decoder architecture. It processes input tokens and predicts the next token in the sequence.

### The Pipeline
1.  **Input:** Text is tokenized into integers.
2.  **Embedding Layer:** Token IDs are mapped to dense vectors + Positional Encodings are added.
3.  **Transformer Blocks (x6):**
    * **Layer Norm (Pre-Norm)**
    * **Causal Self-Attention:** The model looks at past tokens to understand context. A "mask" ensures it cannot cheat by looking at future tokens.
    * **Residual Connection**
    * **Layer Norm**
    * **Feed-Forward Network (MLP):** Expands dimensions (4x) and applies GELU activation.
    * **Residual Connection**
4.  **Output Head:** Final Layer Norm -> Linear projection to vocabulary size -> Softmax -> Next Token Probability.

### ASCII Architecture Diagram
```text
      Output Probabilities
             ‚ñ≤
      (Softmax & Linear)
             ‚ñ≤
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Transformer Block ‚îÇ x N_LAYERS
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ ‚îÇ Feed Forward  ‚îÇ ‚îÇ
    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îÇ    (Add & Norm)   ‚îÇ
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ ‚îÇ Self-Attention‚îÇ ‚îÇ
    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
    ‚îÇ    (Add & Norm)   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚ñ≤
    (Positional Embeddings)
             +
      (Token Embeddings)
             ‚ñ≤
        Input Tokens
````

-----

## üìö Dataset Details

  * **Source:** [TinyStories (HuggingFace)](https://huggingface.co/datasets/roneneldan/TinyStories)
  * **Description:** A synthetic dataset of short stories generated by GPT-3.5/4, constrained to the vocabulary of a 3-4 year old.
  * **Why this dataset?** It proves that the ability of a model to generate coherent text and "reason" is not just about size (parameters), but about the quality and simplicity of the data. Even a small 15M parameter model can master this dataset.

-----

## üõ† Installation & Setup

### Prerequisites

  * Python 3.8+
  * CUDA-enabled GPU (Recommended) or MPS (Mac)

### 1\. Clone the Repository

```bash
git clone [https://github.com/Error404m/custom_llm.git](https://github.com/Error404m/custom_llm.git)
cd MrityunjayaGPT
```

### 2\. Install Dependencies

```bash
pip install torch numpy transformers datasets tiktoken tqdm
```

-----

## üèÉ‚Äç‚ôÇÔ∏è Usage Guide

### Step 1: Data Preparation

Download the TinyStories dataset and tokenize it. This creates `train.bin` and `val.bin` files which are faster to load during training.

```bash
python data/prepare.py
```

### Step 2: Training

Train the model. The script will periodically print the loss and save checkpoints.

```bash
python train.py
```

  * *Note: If you run out of VRAM, lower the `batch_size` in the script.*

### Step 3: Inference (Generation)

Generate a story\! You can provide a custom start prompt.

```bash
python generate.py --prompt "One day, a little cat named" --num_samples 3
```

-----

## ‚öôÔ∏è Configuration & Hyperparameters

The model is configurable via the `config` dictionary in `train.py`. The default "Small" configuration used for MrityunjayaGPT is:

| Hyperparameter | Value | Description |
| :--- | :--- | :--- |
| **Batch Size** | 64 | Number of independent sequences processed in parallel |
| **Block Size** | 256 | Context window (max tokens the model sees at once) |
| **Max Iters** | 5000 | Total training steps |
| **Learning Rate** | 3e-4 | Peak learning rate (with cosine decay) |
| **n\_embd** | 384 | Embedding dimension size |
| **n\_head** | 6 | Number of attention heads |
| **n\_layer** | 6 | Number of Transformer blocks |
| **Dropout** | 0.2 | Regularization to prevent overfitting |
| **Vocab Size** | 50257 | Standard GPT-2 Vocabulary size |

-----

## üìù Sample Generation

**Prompt:** *"The girl found a key"*

**Model Output:**

> The girl found a key on the ground. It was a shiny gold key. "Look mommy\!" she said. "I found a treasure key." Her mom smiled. "That is a nice key," she said. "Maybe it opens a magic box." The girl was happy. She put the key in her pocket and went to play.

*(Note: The model learns grammar, dialogue structure, and simple narrative arcs.)*

-----

## üéì Theoretical Concepts

### Self-Attention Formula

The core of MrityunjayaGPT is the attention mechanism, calculated as:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

Where:

  * **Q (Query):** What am I looking for?
  * **K (Key):** What do I have?
  * **V (Value):** What information do I pass along?
  * **Scaling ($\sqrt{d_k}$):** Stabilizes gradients during training.

### Loss Function

We use **Cross-Entropy Loss**, which measures the difference between the probability distribution predicted by the model and the actual next token in the dataset.

-----


* **Attention Is All You Need** (Vaswani et al., 2017)
    * *The Paper that started it all.* It introduced the Transformer architecture, disposing of Recurrent Neural Networks (RNNs) in favor of the Self-Attention mechanism.
    * [Read Paper](https://arxiv.org/abs/1706.03762)

* **Language Models are Unsupervised Multitask Learners (GPT-2)** (Radford et al., 2019)
    * Describes the specific decoder-only architecture structure (moving Layer Norm to the input of each sub-block) used in this project.
    * [Read Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

### üìú Foundational History (Attention Mechanisms)
* **Neural Machine Translation by Jointly Learning to Align and Translate** (Bahdanau et al., 2014)
    * *Requested Reference.* Known as "Bahdanau Attention." This was one of the first papers to introduce the concept of "attention" in neural networks (specifically for RNNs/LSTMs), laying the groundwork for the Transformer.
    * [Read Paper](https://arxiv.org/abs/1409.0473)

### üß† Dataset & Scaling
* **TinyStories: How Small Can Language Models Be and Still Speak Coherent English?** (Eldan & Li, 2023)
    * The source of the dataset used in this project. It proves that SLMs (Small Language Models) can reason and speak grammatically if trained on simplified, high-quality data.
    * [Read Paper](https://arxiv.org/abs/2305.07759)

* **Language Models are Few-Shot Learners (GPT-3)** (Brown et al., 2020)
    * Demonstrated that scaling up parameters and data (using the same architecture as GPT-2) leads to emergent reasoning capabilities.
    * [Read Paper](https://arxiv.org/abs/2005.14165)

### ‚öôÔ∏è Technical Components
* **Layer Normalization** (Ba, Kiros, & Hinton, 2016)
    * Explains the normalization technique used before the attention and MLP blocks (Pre-Norm) to stabilize training.
    * [Read Paper](https://arxiv.org/abs/1607.06450)

* **Gaussian Error Linear Units (GELU)** (Hendrycks & Gimpel, 2016)
    * The activation function used in the MLP layers (instead of the traditional ReLU), which is standard in GPT models.
    * [Read Paper](https://arxiv.org/abs/1606.08415)

* **Decoupled Weight Decay Regularization (AdamW)** (Loshchilov & Hutter, 2017)
    * The specific optimizer algorithm used to train the model, fixing issues with weight decay in the original Adam optimizer.
    * [Read Paper](https://arxiv.org/abs/1711.05101)

* **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** (Dao et al., 2022)
    * (Advanced) Describes optimizations for making the attention mechanism calculation significantly faster on GPUs.
    * [Read Paper](https://arxiv.org/abs/2205.14135)
      
TinyStories: How Small Can Language Models Be and Still Speak
Coherent English?
Ronen Eldan‚àó and Yuanzhi Li‚Ä†
Microsoft Research

* **Inspired by:** Andrej Karpathy's "NanoGPT" series.


https://arxiv.org/pdf/2305.07759
